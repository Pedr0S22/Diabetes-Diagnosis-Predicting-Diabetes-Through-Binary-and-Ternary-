{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking wich is the best preprocessing methdos combo \n",
    "### Binary and Ternary Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing packages\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "from imblearn.under_sampling import RandomUnderSampler, TomekLinks\n",
    "from itertools import combinations\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, precision_recall_fscore_support\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading dataset and handle a subset of it\n",
    "path_2 = \"datasets/diabetes_binary_health_indicators_BRFSS2015.csv\"\n",
    "path_3 = \"datasets/diabetes_012_health_indicators_BRFSS2015.csv\"\n",
    "\n",
    "df = pd.read_csv(path_2)\n",
    "df_t = pd.read_csv(path_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performing the preprocessing methods combination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Droping columns that are not relevant for the model\n",
    "\n",
    "columns1 = ['ID','Source','End_Lat','End_Lng','End_Time','Start_Time','Description','Airport_Code','Country','Weather_Timestamp'\n",
    "           ,'Nautical_Twilight','Astronomical_Twilight','Timezone','Wind_Direction','Zipcode','Wind_Chill(F)','Temperature(F)',\n",
    "           'Sunrise_Sunset','Street','County','State','City','Precipitation(in)','Bump']\n",
    "\n",
    "#  default\n",
    "columns = ['ID','Source','End_Lat','End_Lng','End_Time','Start_Time','Description','Airport_Code','Country','Weather_Timestamp',\n",
    "           'Civil_Twilight','Nautical_Twilight','Astronomical_Twilight','Timezone','Wind_Direction','Pressure(in)','Zipcode',\n",
    "           'Precipitation(in)','Humidity(%)','Wind_Chill(F)','Temperature(F)','Sunrise_Sunset','Street','County',\n",
    "           'State','City']\n",
    "df1 = df.drop(columns=columns)\n",
    "\n",
    "# MISSING VALUES\n",
    "\n",
    "# Checking Missing Values\n",
    "\n",
    "missing_vals = df1.isna().sum().sort_values(ascending = False) / len(df1) * 100\n",
    "print(missing_vals[missing_vals !=0]) \n",
    "\n",
    "# =============================================================================\n",
    "# ATTENTION! If you use columns1, there are more features with missing values:\n",
    "# Features: Humidity(%), Pressure(in), Civil_Twilight\n",
    "#\n",
    "# Reduce Civil_Twilight to a binary variable\n",
    "#\n",
    "#df[\"Civil_Twilight\"] = df[\"Civil_Twilight\"].map(lambda x: 0 if x == \"Night\" else 1)\n",
    "#\n",
    "# Missing values\n",
    "#\n",
    "#df1.fillna({\n",
    "#    'Humidity(%)': df['Humidity(%)'].median(),\n",
    "#    'Pressure(in)': df['Pressure(in)'].median(),\n",
    "#    'Civil_Twilight': df['Civil_Twilight'].mode()[0]},\n",
    "#    inplace=True)\n",
    "#\n",
    "# =============================================================================\n",
    "\n",
    "# Wind_Speed and Visibility Missing Values\n",
    "\n",
    "df1.fillna({\n",
    "    'Wind_Speed(mph)': df['Wind_Speed(mph)'].median(),\n",
    "    'Visibility(mi)': df['Visibility(mi)'].median()},\n",
    "    inplace=True)\n",
    "\n",
    "# Checking once again the existence of Missing values\n",
    "\n",
    "missing_vals = df1.isna().sum().sort_values(ascending = False) / len(df1) * 100\n",
    "print(df1.shape)\n",
    "print(df1.dtypes)\n",
    "print(missing_vals[missing_vals !=0])\n",
    "\n",
    "# PREPARING THE DATA BEFORE AND AFTER THE DATA SPLITTING\n",
    "\n",
    "# Checking the class distribution before balancing\n",
    "print(\"Before balancing:\", Counter(df1['Severity']))\n",
    "\n",
    "X = df1.drop(columns=['Severity'])\n",
    "y = df1['Severity']\n",
    "\n",
    "# Random Undersampling first to reduce dataset size\n",
    "\n",
    "undersample = RandomUnderSampler(sampling_strategy=0.7, random_state=17)\n",
    "X_resampled, y_resampled = undersample.fit_resample(X, y)\n",
    "\n",
    "f1s_dt, precisions_dt, recalls_dt = [], [], []\n",
    "f1s_knn, precisions_knn, recalls_knn = [], [], []\n",
    "f1s_nb, precisions_nb, recalls_nb = [], [], []\n",
    "\n",
    "n_runs = 5\n",
    "for run in range(n_runs):\n",
    "\n",
    "    # Spltting the data\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=run)\n",
    "\n",
    "    # LOO Encoding\n",
    "\n",
    "    loo_encoder = LeaveOneOutEncoder()\n",
    "    X_train_encoded = loo_encoder.fit_transform(X_train, y_train)\n",
    "    X_test_encoded = loo_encoder.transform(X_test)\n",
    "\n",
    "    # Apply Tomek Links to get better class separation\n",
    "    # If you prefer not use tomek, consider changing y_train to y_tomek\n",
    "    # in model training and changing X_train_encoded to X_tomek in standardization\n",
    "    # And vice-versa!\n",
    "\n",
    "    tomek = TomekLinks()\n",
    "    X_tomek, y_tomek = tomek.fit_resample(X_train_encoded, y_train)  \n",
    "\n",
    "    print(\"After Tomek Links:\", Counter(y_tomek))\n",
    "\n",
    "    # Doing Standardization after splitting to avoid data leakage\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_tomek)\n",
    "    X_test_scaled = scaler.transform(X_test_encoded) \n",
    "\n",
    "    # Using PCA \n",
    "\n",
    "    pca = PCA(n_components=20) \n",
    "\n",
    "    X_train_scaled = pca.fit_transform(X_train_scaled)\n",
    "    X_test_scaled = pca.transform(X_test_scaled)\n",
    "\n",
    "    # Baseline models\n",
    "\n",
    "    dt = DecisionTreeClassifier(max_depth=10)\n",
    "    dt.fit(X_train_scaled, y_tomek)\n",
    "    y_pred_dt = dt.predict(X_test_scaled)\n",
    "\n",
    "    knn = KNeighborsClassifier(n_neighbors=3)\n",
    "    knn.fit(X_train_scaled, y_tomek)\n",
    "    y_pred_knn = knn.predict(X_test_scaled)\n",
    "\n",
    "    nb = GaussianNB()\n",
    "    nb.fit(X_train_scaled, y_tomek)\n",
    "    y_pred_nb = nb.predict(X_test_scaled)\n",
    "\n",
    "    precision_dt, recall_dt, f1_dt, _ = precision_recall_fscore_support(y_test, y_pred_dt, average='binary')\n",
    "    f1s_dt.append(f1_dt)\n",
    "    precisions_dt.append(precision_dt)\n",
    "    recalls_dt.append(recall_dt)\n",
    "\n",
    "    precision_knn, recall_knn, f1_knn, _ = precision_recall_fscore_support(y_test, y_pred_knn, average='binary')\n",
    "    f1s_knn.append(f1_knn)\n",
    "    precisions_knn.append(precision_knn)\n",
    "    recalls_knn.append(recall_knn)\n",
    "\n",
    "    precision_nb, recall_nb, f1_nb, _ = precision_recall_fscore_support(y_test, y_pred_nb, average='binary')\n",
    "    f1s_nb.append(f1_nb)\n",
    "    precisions_nb.append(precision_nb)\n",
    "    recalls_nb.append(recall_nb)\n",
    "\n",
    "results = {\n",
    "        \"Decision Tree\": {\"F1\": round(np.mean(f1s_dt),2), \"Precision\": round(np.mean(precisions_dt),2), \"Recall\": round(np.mean(recalls_dt),2)},\n",
    "        \"KNN\": {\"F1\": round(np.mean(f1s_knn),2), \"Precision\": round(np.mean(precisions_knn),2), \"Recall\": round(np.mean(recalls_knn),2)},\n",
    "        \"Naive Bayes\": {\"F1\": round(np.mean(f1s_nb),2), \"Precision\": round(np.mean(precisions_nb),2), \"Recall\": round(np.mean(recalls_nb),2)}\n",
    "    }\n",
    "pprint(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ternary Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Droping columns that are not relevant for the model\n",
    "\n",
    "columns1 = ['ID','Source','End_Lat','End_Lng','End_Time','Start_Time','Description','Airport_Code','Country','Weather_Timestamp'\n",
    "           ,'Nautical_Twilight','Astronomical_Twilight','Timezone','Wind_Direction','Zipcode','Wind_Chill(F)','Temperature(F)',\n",
    "           'Sunrise_Sunset','Street','County','State','City','Precipitation(in)','Bump']\n",
    "\n",
    "#  default\n",
    "columns = ['ID','Source','End_Lat','End_Lng','End_Time','Start_Time','Description','Airport_Code','Country','Weather_Timestamp',\n",
    "           'Civil_Twilight','Nautical_Twilight','Astronomical_Twilight','Timezone','Wind_Direction','Pressure(in)','Zipcode',\n",
    "           'Precipitation(in)','Humidity(%)','Wind_Chill(F)','Temperature(F)','Sunrise_Sunset','Street','County',\n",
    "           'State','City']\n",
    "df1 = df.drop(columns=columns)\n",
    "\n",
    "# MISSING VALUES\n",
    "\n",
    "# Checking Missing Values\n",
    "\n",
    "missing_vals = df1.isna().sum().sort_values(ascending = False) / len(df1) * 100\n",
    "print(missing_vals[missing_vals !=0]) \n",
    "\n",
    "# =============================================================================\n",
    "# ATTENTION! If you use columns1, there are more features with missing values:\n",
    "# Features: Humidity(%), Pressure(in), Civil_Twilight\n",
    "#\n",
    "# Reduce Civil_Twilight to a binary variable\n",
    "#\n",
    "#df[\"Civil_Twilight\"] = df[\"Civil_Twilight\"].map(lambda x: 0 if x == \"Night\" else 1)\n",
    "#\n",
    "# Missing values\n",
    "#\n",
    "#df1.fillna({\n",
    "#    'Humidity(%)': df['Humidity(%)'].median(),\n",
    "#    'Pressure(in)': df['Pressure(in)'].median(),\n",
    "#    'Civil_Twilight': df['Civil_Twilight'].mode()[0]},\n",
    "#    inplace=True)\n",
    "#\n",
    "# =============================================================================\n",
    "\n",
    "# Wind_Speed and Visibility Missing Values\n",
    "\n",
    "df1.fillna({\n",
    "    'Wind_Speed(mph)': df['Wind_Speed(mph)'].median(),\n",
    "    'Visibility(mi)': df['Visibility(mi)'].median()},\n",
    "    inplace=True)\n",
    "\n",
    "# Checking once again the existence of Missing values\n",
    "\n",
    "missing_vals = df1.isna().sum().sort_values(ascending = False) / len(df1) * 100\n",
    "print(df1.shape)\n",
    "print(df1.dtypes)\n",
    "print(missing_vals[missing_vals !=0])\n",
    "\n",
    "# PREPARING THE DATA BEFORE AND AFTER THE DATA SPLITTING\n",
    "\n",
    "# Checking the class distribution before balancing\n",
    "print(\"Before balancing:\", Counter(df1['Severity']))\n",
    "\n",
    "X = df1.drop(columns=['Severity'])\n",
    "y = df1['Severity']\n",
    "\n",
    "# Random Undersampling first to reduce dataset size\n",
    "\n",
    "undersample = RandomUnderSampler(sampling_strategy=0.7, random_state=17)\n",
    "X_resampled, y_resampled = undersample.fit_resample(X, y)\n",
    "\n",
    "f1s_dt, precisions_dt, recalls_dt = [], [], []\n",
    "f1s_knn, precisions_knn, recalls_knn = [], [], []\n",
    "f1s_nb, precisions_nb, recalls_nb = [], [], []\n",
    "\n",
    "n_runs = 5\n",
    "for run in range(n_runs):\n",
    "\n",
    "    # Spltting the data\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=run)\n",
    "\n",
    "    # LOO Encoding\n",
    "\n",
    "    loo_encoder = LeaveOneOutEncoder()\n",
    "    X_train_encoded = loo_encoder.fit_transform(X_train, y_train)\n",
    "    X_test_encoded = loo_encoder.transform(X_test)\n",
    "\n",
    "    # Apply Tomek Links to get better class separation\n",
    "    # If you prefer not use tomek, consider changing y_train to y_tomek\n",
    "    # in model training and changing X_train_encoded to X_tomek in standardization\n",
    "    # And vice-versa!\n",
    "\n",
    "    tomek = TomekLinks()\n",
    "    X_tomek, y_tomek = tomek.fit_resample(X_train_encoded, y_train)  \n",
    "\n",
    "    print(\"After Tomek Links:\", Counter(y_tomek))\n",
    "\n",
    "    # Doing Standardization after splitting to avoid data leakage\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_tomek)\n",
    "    X_test_scaled = scaler.transform(X_test_encoded) \n",
    "\n",
    "    # Using PCA \n",
    "\n",
    "    pca = PCA(n_components=20) \n",
    "\n",
    "    X_train_scaled = pca.fit_transform(X_train_scaled)\n",
    "    X_test_scaled = pca.transform(X_test_scaled)\n",
    "\n",
    "    # Baseline models\n",
    "\n",
    "    dt = DecisionTreeClassifier(max_depth=10)\n",
    "    dt.fit(X_train_scaled, y_tomek)\n",
    "    y_pred_dt = dt.predict(X_test_scaled)\n",
    "\n",
    "    knn = KNeighborsClassifier(n_neighbors=3)\n",
    "    knn.fit(X_train_scaled, y_tomek)\n",
    "    y_pred_knn = knn.predict(X_test_scaled)\n",
    "\n",
    "    nb = GaussianNB()\n",
    "    nb.fit(X_train_scaled, y_tomek)\n",
    "    y_pred_nb = nb.predict(X_test_scaled)\n",
    "\n",
    "    precision_dt, recall_dt, f1_dt, _ = precision_recall_fscore_support(y_test, y_pred_dt, average='binary')\n",
    "    f1s_dt.append(f1_dt)\n",
    "    precisions_dt.append(precision_dt)\n",
    "    recalls_dt.append(recall_dt)\n",
    "\n",
    "    precision_knn, recall_knn, f1_knn, _ = precision_recall_fscore_support(y_test, y_pred_knn, average='binary')\n",
    "    f1s_knn.append(f1_knn)\n",
    "    precisions_knn.append(precision_knn)\n",
    "    recalls_knn.append(recall_knn)\n",
    "\n",
    "    precision_nb, recall_nb, f1_nb, _ = precision_recall_fscore_support(y_test, y_pred_nb, average='binary')\n",
    "    f1s_nb.append(f1_nb)\n",
    "    precisions_nb.append(precision_nb)\n",
    "    recalls_nb.append(recall_nb)\n",
    "\n",
    "results = {\n",
    "        \"Decision Tree\": {\"F1\": round(np.mean(f1s_dt),2), \"Precision\": round(np.mean(precisions_dt),2), \"Recall\": round(np.mean(recalls_dt),2)},\n",
    "        \"KNN\": {\"F1\": round(np.mean(f1s_knn),2), \"Precision\": round(np.mean(precisions_knn),2), \"Recall\": round(np.mean(recalls_knn),2)},\n",
    "        \"Naive Bayes\": {\"F1\": round(np.mean(f1s_nb),2), \"Precision\": round(np.mean(precisions_nb),2), \"Recall\": round(np.mean(recalls_nb),2)}\n",
    "    }\n",
    "pprint(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
